# ğŸ“Š Engage2Value: From Clicks to Conversions

### Predicting Customer Purchase Value from Multi-Session Digital Behavior

---

**Course:** Machine Learning Practice Project (May 2025 Term)
**Problem Type:** Supervised Regression
**Platform:** Kaggle
**Author:** **Tanush Sudheer Tambe**

---

## ğŸ§  Problem Statement

The goal of this project is to **predict a customerâ€™s purchase value** based on their **multi-session behavior across digital touchpoints**.

The dataset captures detailed session-level interactions including:

* Page views
* Total hits
* Traffic source
* Device & browser information
* Geo-location
* Temporal behavior

The target variable **`purchaseValue`** is:

* Extremely **right-skewed**
* Highly **zero-inflated**
* Influenced by **complex non-linear interactions**

---

## ğŸ¯ Key Challenges

* Heavy **class imbalance** (majority zero purchase sessions)
* Extremely **right-skewed target** (skew > 50)
* High-cardinality categorical features
* Risk of **data leakage via user/session IDs**
* Multicollinearity among strong behavioral features

---

## ğŸ§ª Dataset Overview

| Dataset | Rows    | Columns |
| ------- | ------- | ------- |
| Train   | 116,023 | 52      |
| Test    | 29,006  | 51      |

* Target: `purchaseValue`
* Mixed data types: numerical, categorical, boolean, timestamps
* Large number of **missing and meaningless values**

---

## ğŸ” Exploratory Data Analysis (EDA)

### Numerical Insights

* **Strongest correlated features:**

  * `totalHits`
  * `pageViews`
  * `sessionNumber`
* Target has:

  * Median = 0
  * 75th percentile = 0
  * Extremely large outliers

### Target Transformation

| Method         | Skew                   |
| -------------- | ---------------------- |
| Original       | 53.9                   |
| Power (1/1.54) | 12.15                  |
| log1p          | 1.46 (over-compressed) |

â¡ï¸ **Power transformation with exponent = 1/1.54** was chosen as the best trade-off.

---

## ğŸ§¹ Data Cleaning Strategy

### Removed:

* Columns with **100% null values**
* Constant features (single unique value)
* Features containing only `"not available in demo dataset"`

### Imputation Rules:

* Boolean â†’ logical defaults
* Location â†’ `(not set)`
* Categorical â†’ `(not provided)` / `Other`
* Numerical â†’ mean imputation (pipeline)

---

## âš™ï¸ Feature Engineering

### 1ï¸âƒ£ Session & Interaction Features

* `hits_per_pageview`
* `pageviews_per_hour`
* `session_page_product`
* `bounce_hit_ratio`
* `session_per_hit`

### 2ï¸âƒ£ Behavioral Indicators

* `is_repeat_visitor`
* `is_video_ad_and_bounce`

### 3ï¸âƒ£ Frequency Encoding

* Browser
* City
* Campaign

### 4ï¸âƒ£ Time-Based Features

* Business hour indicators
* Weekend / weekday flags
* Peak month / quarter indicators

---

## ğŸ§ª Leakage-Aware Modeling

Two parallel datasets were maintained:

| Dataset      | Description                          |
| ------------ | ------------------------------------ |
| Leakage-free | Excludes `userId`, `sessionId`       |
| ID-inclusive | Includes ID-based frequency features |

â¡ï¸ This allowed **controlled comparison** between realistic and leaderboard-optimized models.

---

## ğŸ”§ Preprocessing Pipeline

Implemented using **Scikit-Learn Pipelines**:

### Numerical

* Mean Imputation
* RobustScaler

### Categorical

* **Low Cardinality:** OneHotEncoder
* **High Cardinality:** TargetEncoder

All outputs were converted to **fully numeric, float64 matrices**.

---

## ğŸ§  Post-Pipeline Feature Engineering

Derived from scaled features:

* `pageviews_business_hour_scaled`
* `hits_weekend_scaled`
* `efficiency_business`
* `repeat_hits`

---

## âœ‚ï¸ Feature Selection

Correlation-based trimming:

* Retained features with |corr| > 0.01 (no IDs)
* |corr| > 0.07 (ID-inclusive)

This reduced noise and stabilized training.

---

## ğŸ§© Feature Stacking

### 1ï¸âƒ£ K-Means Clustering

* Applied on trimmed, scaled data
* Optimal clusters = **3**
* New feature: `cluster`

### 2ï¸âƒ£ Binary Purchase Classifier

* Model: **LightGBM Classifier**
* Task: Predict `purchaseValue > 0`
* Output: `buy_prob`

**Validation Performance**

* Accuracy: **0.963**
* Precision: **0.881**
* Recall: **0.950**
* F1 Score: **0.914**

â¡ï¸ `buy_prob` became the **most correlated feature** with the target.

---

## ğŸ¤– Models Trained

### Baseline Models

* LightGBM Regressor
* XGBoost Regressor
* ExtraTrees Regressor

### Observations

| Model      | Behavior           |
| ---------- | ------------------ |
| LightGBM   | Underfitting       |
| XGBoost    | Overfitting        |
| ExtraTrees | Severe overfitting |

---

## ğŸ” Hyperparameter Tuning

### Tuned Using:

* `RandomizedSearchCV`
* 3-fold CV
* RÂ² scoring

### Best Single Models

* **Tuned LightGBM (No IDs):** ~0.70 public RÂ²
* **Tuned LightGBM (With IDs):** ~0.728 public RÂ² (leakage)
* **Tuned XGBoost:** Poor generalization

---

## ğŸ§  Ensemble Learning

### VotingRegressor (5 LightGBMs)

* Slight parameter variations
* No ID leakage
* Best balance of bias & variance

â¡ï¸ Achieved **~0.70 public leaderboard RÂ²** without relying on IDs.

---

## ğŸ“¤ Submission Strategy

* Final model trained on full train + validation set
* Power-inverse transformation applied
* Small predictions clipped to zero to reduce noise
* Output saved as `submission.csv`

---

## ğŸ“ˆ Final Results (Kaggle)

| Model                     | Public RÂ²  |
| ------------------------- | ---------- |
| Tuned LightGBM (No IDs)   | ~0.698     |
| Tuned LightGBM (With IDs) | **~0.728** |
| Ensemble (No IDs)         | **~0.70**  |

---

## ğŸ Key Takeaways

* Target transformation is **critical** for skewed regression
* Feature stacking (cluster + buy_prob) adds **major signal**
* ID features improve leaderboard score but introduce leakage
* Ensemble models provide best **real-world generalization**

---

## ğŸ§‘â€ğŸ’» How to Run

This project is implemented as a **single Kaggle notebook**.

1. Open notebook on Kaggle
2. Attach competition dataset
3. Run cells sequentially
4. Submission file is generated automatically

---

## ğŸ“œ License

This project is released under the **MIT License** and is intended for **educational and research purposes**.

---

Just tell me ğŸ‘
